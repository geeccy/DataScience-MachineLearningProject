---
title: "Machine Learning Project - Bicep Curls"
output: html_document
---

```{r setup, include=FALSE cache=TRUE}
```

### Introduction

Six participants wore accelerometers on the belt, forearm, arm, and dumbbell, and were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.  

The study is available [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) and the datasets have been divided into [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).  

We will build several machine learning models from the CARET package and compare their performances. We will choose a model to produce predictions on the test dataset.  

### Load the library and datasets ###
```{r library, warning=FALSE}
library(tidyverse)
library(caret)

training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

### Data Cleansing ###

After visually examining the datasets we can see they contain many variables which are NA and #DIV/0. The training algorithms won't be able to deal with these, so we will remove them. Some of the variables (e.g. max_roll_belt) are aggregates of the measurement window and only have values when new_window = yes. The test dataset doesn't have any values where new_window = yes so these aggregate variable won't be used in prediction anyway, we will remove them too.
```{r dataclean}
#classe is the outcome we are trying to predict. Rename to 'class'.
training$class <- factor(training$classe)

#select only variables with valid values.
training <- training %>% 
  select(c(user_name, roll_belt, pitch_belt, yaw_belt, total_accel_belt, gyros_belt_x, gyros_belt_y, gyros_belt_z, accel_belt_x, accel_belt_y, accel_belt_z, magnet_belt_x, magnet_belt_y, magnet_belt_z, roll_arm, pitch_arm, yaw_arm, total_accel_arm, gyros_arm_x, 
gyros_arm_y, gyros_arm_z, accel_arm_x, accel_arm_y, accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z, roll_dumbbell, pitch_dumbbell, 
yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x, gyros_dumbbell_y, gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z, magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, roll_forearm, pitch_forearm, yaw_forearm, total_accel_forearm, 
gyros_forearm_x, gyros_forearm_y, gyros_forearm_z, accel_forearm_x, accel_forearm_y, accel_forearm_z, magnet_forearm_x, magnet_forearm_y, 
magnet_forearm_z, class))

testing <- testing %>% 
  select(c(user_name, roll_belt, pitch_belt, yaw_belt, total_accel_belt, gyros_belt_x, gyros_belt_y, gyros_belt_z, accel_belt_x, accel_belt_y, accel_belt_z, magnet_belt_x, magnet_belt_y, magnet_belt_z, roll_arm, pitch_arm, yaw_arm, total_accel_arm, gyros_arm_x, 
gyros_arm_y, gyros_arm_z, accel_arm_x, accel_arm_y, accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z, roll_dumbbell, pitch_dumbbell, 
yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x, gyros_dumbbell_y, gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z, magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, roll_forearm, pitch_forearm, yaw_forearm, total_accel_forearm, 
gyros_forearm_x, gyros_forearm_y, gyros_forearm_z, accel_forearm_x, accel_forearm_y, accel_forearm_z, magnet_forearm_x, magnet_forearm_y, 
magnet_forearm_z))

#Check any NA values left
sum(is.na(training))
sum(is.na(testing))
```  


### Build the validation set  

The testing dataset doesn't have the outcomes, so we cannot use it to check the models. We will take 20% sample from our training dataset to be the validation dataset. This can be used to estimate out-of-sample errors.

```{r dataset cache=TRUE}
set.seed(333)
inTrain <- createDataPartition(y=training$class, p = 0.8, list=FALSE)
trainsub <- data.frame(training[inTrain,])  #training subset
validation <- data.frame(training[-inTrain,]) #validation subset
```

### Building Models

We will try a few methods in the caret package and see how they perform on both training and validation datasets.

#### Create random forest model

We will use a faster implementation of random forest called *ranger*.
```{r ranger cache=TRUE}
library(ranger)
set.seed(333)
modelranger <- train(class~., method="ranger", data=trainsub, trControl=trainControl(number=4))

#time to build model
#   user  system elapsed 
#1781.94    2.04  178.33 
```

#### Accuracy of random forest model

*Accuracy on training dataset*
```{r}
pred_train_ranger <- predict(modelranger, trainsub[,-54])
confusionMatrix(pred_train_ranger, trainsub[,54])
```
The random forest model gave a perfect accuracy of 100%, all the classes are correctly grouped in the confusion matrix, going down the diagonal.  

*Accuracy on validation dataset*
```{r}
pred_valid_ranger <- predict(modelranger, validation[,-54])
confusionMatrix(pred_valid_ranger, validation[,54])
```
The accuracy on the validation set only dropped slightly, and the confusion matrix still has a strong diagonal pattern.  


#### Create boosted tree mdoel

Next we will try a boosted tree model using the GBM method.
```{r gbm cache=TRUE}
set.seed(333)
modelgbm <- train(class~., method="gbm", data=trainsub, verbose=FALSE)
```

The boosted tree model took over 22 minutes to build, according to the system.time function.
```{r}
#system.time(modelgbm <- train(class~., method="gbm", data=trainsub, verbose=FALSE))
#user  system elapsed 
#1354.81    2.55 1359.59 
```

#### Accuracy of boosted tree model

*Accuracy on training dataset*  
```{r}
pred_train_gbm <- predict(modelgbm, trainsub[,-54])
confusionMatrix(pred_train_gbm, trainsub[,54])
#Accuracy 97.3%
```
The model has 97.3% accuracy, less than the random forest model. It still shows a diagonal pattern in the confusion matrix.

*Accuracy on validation dataset*
```{r}
pred_valid_gbm <- predict(modelgbm, validation[,-54])
confusionMatrix(pred_valid_gbm, validation[,54])
#Accuracy 96.1%
```
The accuracy dropped slightly to 96.1. The prediction for class C shows more variance.


#### Create bagged tree model
```{r bag cache=TRUE}
modelbag <- train(class~., method="treebag", data=trainsub)
```

The model took just over 4 minutes to build.
```{r}
#system.time(modelbag <- train(class~., method="treebag", data=trainsub))
#   user  system elapsed 
# 251.45    0.03  251.61
```

#### Accuracy of bagged tree model

*Accuracy on training dataset*
```{r}
pred_train_treebag <- predict(modelbag, trainsub[,-54])
confusionMatrix(pred_train_treebag, trainsub[,54])
#Accuracy 99.9%
```
The model has accuracy over 99.9%, and shows strong diagonal pattern in the confusion matrix.


*Accuracy on validation dataset*
```{r}
pred_valid_treebag <- predict(modelbag, validation[,-54])
confusionMatrix(pred_valid_treebag, validation[,54])
#Accuracy 98.5
```
Slight drop in out-of-sample accuracy but still a strong 98.5%.  


### Model Comparison

Summarise the performance of each model.
```{r comparison}

acc_train_ranger <- round(postResample(pred_train_ranger, trainsub[,54])[1], 4)
acc_valid_ranger <- round(postResample(pred_valid_ranger, validation[,54])[1], 4)
acc_drop_ranger <- round(acc_train_ranger - acc_valid_ranger,4)

print(paste("Random Forest:", "Training set accuracy", acc_train_ranger, 
            ", Validation set accuracy", acc_valid_ranger, 
            ", Accuracy drop", acc_drop_ranger))

acc_train_gbm <- round(postResample(pred_train_gbm, trainsub[,54])[1], 4)
acc_valid_gbm <- round(postResample(pred_valid_gbm, validation[,54])[1], 4)
acc_drop_gbm <- round(acc_train_gbm - acc_valid_gbm,4)

print(paste("Boosted Tree:", "Training set accuracy", acc_train_gbm, 
            ", Validation set accuracy", acc_valid_gbm, 
            ", Accuracy drop", acc_drop_gbm))

acc_train_treebag <- round(postResample(pred_train_treebag, trainsub[,54])[1], 4)
acc_valid_treebag <- round(postResample(pred_valid_treebag, validation[,54])[1], 4)
acc_drop_treebag <- round(acc_train_treebag - acc_valid_treebag,4)

print(paste("Bagged Tree:", "Training set accuracy", acc_train_treebag, 
            ", Validation set accuracy", acc_valid_treebag, 
            ", Accuracy drop", acc_drop_treebag))

```

```{r eval=FALSE, include=FALSE}
Accuracy_train_set <- c(
  postResample(pred_train_ranger, trainsub[,54])[1],
  postResample(pred_train_gbm, trainsub[,54])[1],
  postResample(pred_train_treebag, trainsub[,54])[1]
)

Accuracy_validation_set <- c(
  postResample(pred_valid_ranger, validation[,54])[1],
  postResample(pred_valid_gbm, validation[,54])[1],
  postResample(pred_valid_treebag, validation[,54])[1]
)

ModelName <- c("Random Forest", "Boosted Tree", "Bagged Tree")
models <- data.frame(names, Accuracy_train_set, Accuracy_validation_set)
models <- table(names, Accuracy_train_set, Accuracy_validation_set)
models$Accuracy_drop <- models[,2] - models[,3]
#models$Build_time_in_minutes <- c(3253.97/60, 1354.81/60, 251.45/60)

print(models)
```
Random forest is the winner in accuracy for both datasets, and also has the lowest out-of-sample error rate. However it was also the most time-consuming model to build. Bagged tree has slightly more accuracy than the Boosted tree, and required significantly less time to build. We will use *Random forest* as the model to produce predictions on the testing dataset.  

### Prediction on testing dataset
Create prediction based on random forest model.
```{r output}
pred_test_ranger <- predict(modelranger, testing)
```

In fact, if we run each of the models on the test dataset, they all produce the same predictions.
```{r}
pred_test_gbm <- predict(modelgbm, testing)
pred_test_treebag <- predict(modelbag, testing)
t(data.frame(pred_test_ranger, pred_test_gbm, pred_test_treebag))
```
